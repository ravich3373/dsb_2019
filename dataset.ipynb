{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#import fastText\n",
    "#from fastText import load_model\n",
    "import gc\n",
    "import re\n",
    "tqdm.pandas()\n",
    "from gensim.models import KeyedVectors\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from collections import Iterable\n",
    "import math\n",
    "from torch import LongTensor,Tensor\n",
    "from torch.nn import CrossEntropyLoss\n",
    "#from fastai import *\n",
    "import warnings\n",
    "from typing import *\n",
    "from dataclasses import dataclass,field\n",
    "from torch import optim\n",
    "import os\n",
    "from abc import abstractmethod\n",
    "from functools import partial\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Model = nn.Module\n",
    "Floats = Union[float, Collection[float]]\n",
    "ArgStar = Collection[Any]\n",
    "Tensors = Union[Tensor, Collection['Tensors']]\n",
    "Rank0Tensor = NewType('OneEltTensor', Tensor)\n",
    "PathOrStr = Union[Path,str]\n",
    "TItem = TypeVar('TItem')\n",
    "TfmCallable = Callable[[TItem],TItem]\n",
    "TfmList = Union[TfmCallable, Collection[TfmCallable]]\n",
    "ModuleList = Collection[nn.Module]\n",
    "SplitFuncOrIdxList = Union[Callable, Collection[ModuleList]]\n",
    "class ItemBase():\n",
    "    \"All transformable dataset items use this type\"\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def device(self): pass\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def data(self): pass\n",
    "\n",
    "ItemsList = Collection[Union[Tensor,ItemBase,'ItemsList',float,int]]\n",
    "def num_cpus()->int:\n",
    "    \"Get number of cpus\"\n",
    "    try:                   return len(os.sched_getaffinity(0))\n",
    "    except AttributeError: return os.cpu_count()\n",
    "default_cpus = min(16, num_cpus())\n",
    "StartOptEnd=Union[float,Tuple[float,float]]\n",
    "bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n",
    "ModuleList = Collection[nn.Module]\n",
    "ParamList = Collection[nn.Parameter]\n",
    "def is_tuple(x:Any)->bool: return isinstance(x, tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>game_session</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>event_data</th>\n",
       "      <th>installation_id</th>\n",
       "      <th>event_count</th>\n",
       "      <th>event_code</th>\n",
       "      <th>game_time</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27253bdc</td>\n",
       "      <td>45bb1e1b6b50c07b</td>\n",
       "      <td>2019-09-06T17:53:46.937Z</td>\n",
       "      <td>{\"event_code\": 2000, \"event_count\": 1}</td>\n",
       "      <td>0001e90f</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>Welcome to Lost Lagoon!</td>\n",
       "      <td>Clip</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27253bdc</td>\n",
       "      <td>17eeb7f223665f53</td>\n",
       "      <td>2019-09-06T17:54:17.519Z</td>\n",
       "      <td>{\"event_code\": 2000, \"event_count\": 1}</td>\n",
       "      <td>0001e90f</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>Magma Peak - Level 1</td>\n",
       "      <td>Clip</td>\n",
       "      <td>MAGMAPEAK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id      game_session                 timestamp  \\\n",
       "0  27253bdc  45bb1e1b6b50c07b  2019-09-06T17:53:46.937Z   \n",
       "1  27253bdc  17eeb7f223665f53  2019-09-06T17:54:17.519Z   \n",
       "\n",
       "                               event_data installation_id  event_count  \\\n",
       "0  {\"event_code\": 2000, \"event_count\": 1}  0001e90f        1             \n",
       "1  {\"event_code\": 2000, \"event_count\": 1}  0001e90f        1             \n",
       "\n",
       "   event_code  game_time                    title  type      world  \n",
       "0  2000        0          Welcome to Lost Lagoon!  Clip  NONE       \n",
       "1  2000        0          Magma Peak - Level 1     Clip  MAGMAPEAK  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NONE', 'MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['world'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_ttop_cty = df.query('world== \"TREETOPCITY\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_cryst_rulz = df_ttop_cty[df_ttop_cty['title'] == 'Crystals Rule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab = df_cryst_rulz.event_code.unique()\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2000, 2010, 2020, 2030, 3010, 3020, 3021, 3110, 3120, 3121, 4010,\n",
       "       4020, 4050, 4070, 4090])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab_ = df.event_code.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2000, 3010, 3110, 4070, 4090, 4030, 4035, 4021, 4020, 4010, 2080,\n",
       "       2083, 2040, 2020, 2030, 3021, 3121, 2050, 3020, 3120, 2060, 2070,\n",
       "       4031, 4025, 5000, 5010, 2081, 2025, 4022, 2035, 4040, 4100, 2010,\n",
       "       4110, 4045, 4095, 4220, 2075, 4230, 4235, 4080, 4050])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3145"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grps = df_cryst_rulz.groupby(\"game_session\")\n",
    "len(grps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "codes_by_session = defaultdict(lambda : [])\n",
    "for name,grp in grps:\n",
    "    grp_ = grp.sort_values(by='event_count').reset_index()\n",
    "    codes_by_session[name] = grp_['event_code'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3145"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(codes_by_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for key,value in codes_by_session.items():\n",
    "    dat = np.array([9998])\n",
    "    dat = np.concatenate((dat,value))\n",
    "    dat = np.append(dat,9999)\n",
    "    data.append(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3145"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab2index = defaultdict(lambda : -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i,code in enumerate(vocab):\n",
    "    vocab2index[code] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab2index[9998] = 15 # start of events id\n",
    "vocab2index[9999] = 16 # end of event ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index2vocab = { i:v for v,i in vocab2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### lens = []\n",
    "for arr in data:\n",
    "    lens.append(len(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### min(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### data_pad = []\n",
    "for arr in data:\n",
    "    len_ = len(arr)\n",
    "    if len_<2376:\n",
    "        append_sz = 2376-len_\n",
    "        z = np.zeros(append_sz)\n",
    "        zz = np.append(arr,z)\n",
    "        data_pad.append(zz)\n",
    "    elif len_==2376:\n",
    "        data_pad.append(arr)\n",
    "    else:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3145"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9998, 2000, 4010, 3010, 3110, 2020, 3010, 3110, 4050, 4070, 4020,\n",
       "       3010, 3110, 3010, 3110, 3010, 3110, 3021, 3121, 2030, 2020, 3010,\n",
       "       3110, 4020, 3010, 3110, 3010, 3110, 3010, 3110, 3020, 4070, 3120,\n",
       "       4020, 3010, 3110, 3010, 3110, 3021, 3121, 2030, 2020, 3010, 3110,\n",
       "       4020, 3010, 3110, 3010, 3110, 3021, 3121, 2030, 2020, 3010, 3110,\n",
       "       4020, 3010, 3110, 3010, 3110, 3010, 3110, 3021, 3121, 2030, 2020,\n",
       "       3010, 4070, 3110, 4020, 3010, 3110, 3010, 3110, 3010, 3110, 3021,\n",
       "       3121, 2030, 2020, 3010, 3110, 4020, 3010, 3110, 3010, 3110, 3010,\n",
       "       3110, 3010, 3110, 3020, 4070, 4070, 4070, 4070, 3120, 4020, 3010,\n",
       "       4070, 3110, 3010, 3110, 3010, 3110, 3021, 3121, 2030, 2020, 3010,\n",
       "       3110, 4020, 3010, 3110, 3010, 3110, 3010, 3110, 3010, 3110, 3020,\n",
       "       4070, 3120, 4020, 3010, 3110, 3010, 3110, 3010, 3110, 3010, 3110,\n",
       "       3010, 3110, 3021, 3121, 2030, 2020, 3010, 4070, 3110, 4020, 3010,\n",
       "       4070, 3110, 3010, 3110, 3010, 3110, 3010, 3110, 3010, 3110, 3021,\n",
       "       3121, 2030, 2020, 3010, 4070, 3110, 4020, 3010, 3110, 3010, 3110,\n",
       "       3010, 3110, 3010, 3110, 3010, 3110, 3020, 4070, 4070, 4070, 3120,\n",
       "       4020, 3010, 4070, 3110, 3010, 3110, 3010, 3110, 3010, 3110, 3021,\n",
       "       3121, 2030, 3021, 3121, 2020, 3010, 3110, 9999])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#vocab2index[0] = 0\n",
    "#index2vocab[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_ind = []\n",
    "for i in range(len(data)):\n",
    "    dat = []\n",
    "    for val in data[i]:   \n",
    "        dat.append(vocab2index[int(val)])\n",
    "    data_ind.append(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3145, 3145)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data),len(data_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rnd_ind = np.random.permutation(range(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cut = int(0.75*len(data))\n",
    "rnd_ind_trn = rnd_ind[:cut]\n",
    "rnd_ind_tst = rnd_ind[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_ind = np.array(data_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2358, 787)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_trn = data_ind[rnd_ind_trn]\n",
    "data_tst = data_ind[rnd_ind_tst]\n",
    "len(data_trn),len(data_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_trn_ = np.concatenate(data_trn)\n",
    "data_tst_ = np.concatenate(data_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343425, 116717)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_trn_),len(data_tst_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelLoader():\n",
    "    \"Creates a dataloader with bptt slightly changing.\"\n",
    "    \n",
    "    def __init__(self, nums:np.ndarray, bs:int=64, bptt:int=70, backwards:bool=False):\n",
    "        self.bs,self.bptt,self.backwards = bs,bptt,backwards\n",
    "        self.data = self.batchify(nums)\n",
    "        self.first,self.i,self.iter = True,0,0\n",
    "        self.n = len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i,self.iter = 0,0\n",
    "        while self.i < self.n-1 and self.iter<len(self):\n",
    "            if self.first and self.i == 0: self.first,seq_len = False,self.bptt + 25\n",
    "            else:\n",
    "                bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.\n",
    "                seq_len = max(5, int(np.random.normal(bptt, 5)))\n",
    "            res = self.get_batch(self.i, seq_len)\n",
    "            self.i += seq_len\n",
    "            self.iter += 1\n",
    "            yield res\n",
    "\n",
    "    def __len__(self) -> int: return (self.n-1) // self.bptt\n",
    "\n",
    "    def batchify(self, data:np.ndarray) -> LongTensor:\n",
    "        \"Splits the data in batches.\"\n",
    "        print(data.shape)\n",
    "        nb = data.shape[0] // self.bs\n",
    "        data = np.array(data[:nb*self.bs]).reshape(self.bs, -1).T\n",
    "        print(data.shape)\n",
    "        if self.backwards: data=data[::-1]\n",
    "        return LongTensor(data)\n",
    "\n",
    "    def get_batch(self, i:int, seq_len:int) -> LongTensor:\n",
    "        \"Gets a batch of length `seq_len`\"\n",
    "        seq_len = min(seq_len, len(self.data) - 1 - i)\n",
    "        return self.data[i:i+seq_len], self.data[i+1:i+1+seq_len].contiguous().view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        embed_size = 50\n",
    "        lstm_hidden_size = 120\n",
    "        gru_hidden_size = 60\n",
    "        #self.gru_hidden_size = gru_hidden_size\n",
    "        self.embedding = nn.Embedding(17, embed_size)\n",
    "        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        #self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size, lstm_hidden_size, bidirectional=False, batch_first=False)\n",
    "        #self.gru = nn.GRU(lstm_hidden_size*2, gru_hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(lstm_hidden_size, 60)\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(60, 17)\n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        #h_embedding = torch.unsqueeze(h_embedding.transpose(1, 2), 2)\n",
    "        #h_embedding = torch.squeeze(self.embedding_dropout(h_embedding)).transpose(1, 2)\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        #h_gru, hh_gru = self.gru(h_lstm)\n",
    "        #hh_gru = hh_gru.view(-1, self.gru_hidden_size*2)\n",
    "        #avg_pool = torch.mean(h_gru, 1)\n",
    "        #max_pool, _ = torch.max(h_gru, 1)\n",
    "        #conc = torch.cat((hh_gru, avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(h_lstm))\n",
    "        #conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    _order = 0\n",
    "    def set_runner(self, run): self.run = run\n",
    "    def __getattr__(self, k): return getattr(self.run, k)\n",
    "    @property\n",
    "    def name(self):\n",
    "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
    "        return camel2snake(name or 'callback')\n",
    "    def __call__(self, cb_name):\n",
    "        f = getattr(self, cb_name, None)\n",
    "        if f and f(): return True\n",
    "        return False\n",
    "class TrainEvalCallback(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.run.n_epochs = 0\n",
    "        self.run.n_iter = 0\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.run.n_epochs+=1./self.iters\n",
    "        self.run.n_iter+=1\n",
    "    def begin_epoch(self):\n",
    "        self.run.n_epochs = self.epoch\n",
    "        self.model.train()\n",
    "        self.run.in_train = True\n",
    "    def begin_validate(self):\n",
    "        self.model.eval()\n",
    "        self.run.in_train = False\n",
    "\n",
    "class CancelTrainException(Exception): pass\n",
    "class CancelEpochException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "\n",
    "ListOrItem = Union[Collection[Any],int,float,str]\n",
    "OptListOrItem = Optional[ListOrItem]\n",
    "def listify(p:OptListOrItem=None, q:OptListOrItem=None):\n",
    "    \"Makes `p` same length as `q`\"\n",
    "    if p is None: p=[]\n",
    "    elif not isinstance(p, Iterable): p=[p]\n",
    "    n = q if type(q)==int else len(p) if q is None else len(q)\n",
    "    if len(p)==1: p = p * n\n",
    "    assert len(p)==n, f'List len mismatch ({len(p)} vs {n})'\n",
    "    return list(p)\n",
    "class Runner():\n",
    "    def __init__(self, cbs=None, cb_funcs=None):\n",
    "        cbs = listify(cbs)\n",
    "        for cbf in listify(cb_funcs):\n",
    "            cb = cbf()\n",
    "            setattr(self, cb.name, cb)\n",
    "            cbs.append(cb)\n",
    "        self.stop, self.cbs = False, [TrainEvalCallback()] + cbs\n",
    "    @property\n",
    "    def opt(self): return self.learn.opt\n",
    "    @property\n",
    "    def model(self): return self.learn.model\n",
    "    @property\n",
    "    def loss_func(self): return self.learn.loss_func\n",
    "    @property\n",
    "    def data(self): return self.learn.data\n",
    "    \n",
    "    def one_batch(self, xb, yb):\n",
    "        try: \n",
    "            self.xb, self.yb = xb, yb\n",
    "            self('begin_batch')\n",
    "            self.pred = self.model(self.xb)\n",
    "            self('after_pred')\n",
    "            self.loss = self.loss_func(self.pred, self.yb)\n",
    "            self('after_loss')\n",
    "            if not self.in_train: return\n",
    "            self.loss.backward()\n",
    "            self('after_backward')\n",
    "            self.opt.step()\n",
    "            self('after_step')\n",
    "            self.opt.zero_grad()\n",
    "        except CancelBatchException: self('after_cancel_batch')\n",
    "        finally: self('after_batch')\n",
    "    \n",
    "    def all_batches(self, dl):\n",
    "        self.iters = len(dl)\n",
    "        try:\n",
    "            for xb, yb in progress_bar(dl, leave=False): self.one_batch(xb, yb)\n",
    "        except CancelEpochException: self('after_cancel_epoch')\n",
    "    def fit(self, epochs, learn):\n",
    "        self.epochs, self.learn, self.loss = epochs, learn, torch.tensor(0.)\n",
    "        try: \n",
    "            for cb in self.cbs: cb.set_runner(self)\n",
    "            self('begin_fit')\n",
    "            for epoch in range(epochs):\n",
    "                self.epoch = epoch\n",
    "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
    "                with torch.no_grad():\n",
    "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
    "                self('after_epoch')\n",
    "        except CancelTrainException: self('after_cancel_train')\n",
    "        finally:\n",
    "            self('after_fit')\n",
    "            self.learn = None\n",
    "    def __call__(self, cb_name):\n",
    "        res = False\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) and res\n",
    "        return res\n",
    "#class Learner():\n",
    "#    def __init__(self, model, opt, loss_func, data):\n",
    "#        self.model, self.opt, self.loss_func, self.data = model, opt, loss_func, data\n",
    "def get_model(data, lr=0.005):\n",
    "    # ravi error\n",
    "    model = Net(embedding_matrix).to(device)\n",
    "    return model, torch.optim.Adam(model.parameters(), lr)\n",
    "#class DataBunch():\n",
    "#    def __init__(self, train_dl, valid_dl):\n",
    "#        self.train_dl, self.valid_dl = train_dl, valid_dl\n",
    "#    @property\n",
    "#    def train_ds(self): return self.train_dl.dataset\n",
    "#    \n",
    "#    @property\n",
    "#    def valid_ds(self): return self.valid_dl.dataset\n",
    "def data_collate(batch:ItemsList)->Tensor:\n",
    "    \"Convert `batch` items to tensor data\"\n",
    "    return torch.utils.data.dataloader.default_collate(to_data(batch))\n",
    "\n",
    "@dataclass\n",
    "class DeviceDataLoader():\n",
    "    \"Binds a `DataLoader` to a `torch.device`\"\n",
    "    dl: DataLoader\n",
    "    device: torch.device\n",
    "    tfms: List[Callable]=None\n",
    "    collate_fn: Callable=data_collate\n",
    "    def __post_init__(self):\n",
    "        self.dl.collate_fn=self.collate_fn # are we using this?\n",
    "        self.tfms = listify(self.tfms)\n",
    "\n",
    "    def __len__(self)->int: return len(self.dl)\n",
    "    def __getattr__(self,k:str)->Any: return getattr(self.dl, k)\n",
    "\n",
    "    def add_tfm(self,tfm:Callable)->None:    self.tfms.append(tfm)\n",
    "    def remove_tfm(self,tfm:Callable)->None: self.tfms.remove(tfm)\n",
    "\n",
    "    def proc_batch(self,b:Tensor)->Tensor:\n",
    "        \"Proces batch `b` of `TensorImage`\"\n",
    "        b = to_device(b, self.device)\n",
    "        for f in listify(self.tfms): b = f(b)\n",
    "        return b\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"Process and returns items from `DataLoader`\"\n",
    "        self.gen = map(self.proc_batch, self.dl)\n",
    "        return iter(self.gen)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, dataset:Dataset, bs:int=1, shuffle:bool=False, device:torch.device=default_device,\n",
    "               tfms:TfmList=tfms, num_workers:int=default_cpus, collate_fn:Callable=data_collate, **kwargs:Any):\n",
    "        \"Create DeviceDataLoader from `dataset` with `batch_size` and `shuffle`: processs using `num_workers`\"\n",
    "        return cls(DataLoader(dataset, batch_size=bs, shuffle=shuffle, num_workers=num_workers, **kwargs),\n",
    "                   device=device, tfms=tfms, collate_fn=collate_fn)\n",
    "\n",
    "class DataBunch():\n",
    "    \"Bind `train_dl`,`valid_dl` and`test_dl` to `device`. tfms are DL tfms (normalize). `path` is for models.\"\n",
    "    def __init__(self, train_dl:DataLoader, valid_dl:DataLoader, test_dl:Optional[DataLoader]=None,\n",
    "                 device:torch.device=None, tfms:Optional[Collection[Callable]]=None, path:PathOrStr='.'):\n",
    "        \"Bind `train_dl`,`valid_dl` and`test_dl` to `device`. tfms are DL tfms (normalize). `path` is for models.\"\n",
    "        self.device = default_device if device is None else device\n",
    "        self.train_dl = DeviceDataLoader(train_dl, self.device, tfms=tfms)\n",
    "        self.valid_dl = DeviceDataLoader(valid_dl, self.device, tfms=tfms)\n",
    "        self.test_dl  = DeviceDataLoader(test_dl,  self.device, tfms=tfms) if test_dl else None\n",
    "        self.path = Path(path)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None,\n",
    "               path='.', bs=64, ds_tfms=None, num_workers=default_cpus,\n",
    "               tfms=None, device=None, size=None, **kwargs)->'DataBunch':\n",
    "        \"`DataBunch` factory. `bs` batch size, `ds_tfms` for `Dataset`, `tfms` for `DataLoader`\"\n",
    "        datasets = [train_ds,valid_ds]\n",
    "        if test_ds is not None: datasets.append(test_ds)\n",
    "        if ds_tfms: datasets = transform_datasets(*datasets, tfms=ds_tfms, size=size, **kwargs)\n",
    "        dls = [DataLoader(*o, num_workers=num_workers) for o in\n",
    "               zip(datasets, (bs,bs*2,bs*2), (True,False,False))]\n",
    "        return cls(*dls, path=path, device=device, tfms=tfms)\n",
    "\n",
    "    def __getattr__(self,k)->Any: return getattr(self.train_ds, k)\n",
    "    def holdout(self, is_test:bool=False)->DeviceDataLoader:\n",
    "        \"Returns correct holdout `Dataset` for test vs validation (`is_test`)\"\n",
    "        return self.test_dl if is_test else self.valid_dl\n",
    "\n",
    "    @property\n",
    "    def train_ds(self)->Dataset: return self.train_dl.dl.dataset\n",
    "    @property\n",
    "    def valid_ds(self)->Dataset: return self.valid_dl.dl.dataset\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class AvgStats():\n",
    "    def __init__(self, metrics, in_train): self.metrics, self.in_train = listify(metrics), in_train\n",
    "    def reset(self):\n",
    "        self.tot_loss, self.count = 0., 0\n",
    "        self.tot_mets = [0.]*len(self.metrics)\n",
    "    @property\n",
    "    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
    "    @property\n",
    "    def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if not self.count: return ''\n",
    "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
    "    def accumulate(self, run):\n",
    "        bn = run.xb.shape[0]\n",
    "        self.tot_loss+=run.loss*bn\n",
    "        self.count+=bn\n",
    "        for i, m in enumerate(self.metrics):\n",
    "            self.tot_mets[i]+=m(run.pred, run.yb)*bn\n",
    "class AvgStatsCallBack(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats, self.valid_stats = AvgStats(metrics, True), AvgStats(metrics, False)\n",
    "    def begin_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad(): stats.accumulate(self.run)\n",
    "    def after_epoch(self):\n",
    "        print(self.train_stats)\n",
    "        print(self.valid_stats)\n",
    "class Recorder(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.lrs = [[] for _ in self.opt.param_groups]\n",
    "        self.losses = []\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        for pg, lr in zip(self.opt.param_groups, self.lrs): lr.append(pg['lr'])\n",
    "        self.losses.append(self.loss.detach().cpu())\n",
    "    \n",
    "    def plot_lr(self, pgid=-1): plt.plot(self.lrs[pgid])\n",
    "    def plot_loss(self, skip_last=0): plt.plot(self.losse[:len(self.losses)-skip_last])\n",
    "    def plot(self, skip_last=0, pgid=-1):\n",
    "        losses = [o.item() for o in self.losses]\n",
    "        lrs = self.lrs[pgid]\n",
    "        n = len(losses)-skip_last\n",
    "        plt.xscale('log')\n",
    "        plt.plot(lrs[:n], losses[:n])\n",
    "        \n",
    "        \n",
    "def roc(out, y):\n",
    "    score = roc_auc_score(y.cpu().detach().numpy(), out.cpu().detach().numpy())\n",
    "    return score\n",
    "class ParamScheduler(Callback):\n",
    "    _order = 1\n",
    "    def __init__(self, pname, sched_funcs): self.pname, self.sched_funcs = pname, sched_funcs\n",
    "    def begin_fit(self):\n",
    "        if not isinstance(self.sched_funcs, (list, tuple)):\n",
    "            self.sched_funcs = [self.sched_funcs]*len(self.opt.param_groups)\n",
    "    def set_param(self):\n",
    "        assert len(self.opt.param_groups)==len(self.sched_funcs)\n",
    "        for pg, f in zip(self.opt.param_groups, self.sched_funcs):\n",
    "            pg[self.pname] = f(self.n_epochs/self.epochs)\n",
    "    def begin_batch(self):\n",
    "        if self.in_train: self.set_param()\n",
    "class LR_Find(Callback):\n",
    "    _order = 1\n",
    "    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n",
    "        self.max_iter, self.min_lr, self.max_lr = max_iter, min_lr, max_lr\n",
    "        self.best_loss = 1e9\n",
    "    def begin_batch(self):\n",
    "        if not self.in_train: return\n",
    "        pos = self.n_iter/self.max_iter\n",
    "        lr = self.min_lr*(self.max_lr/self.min_lr)**pos\n",
    "        for pg in self.opt.param_groups: pg['lr'] = lr\n",
    "    def after_step(self):\n",
    "        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:\n",
    "            raise CancelTrainException()\n",
    "        if self.loss<self.best_loss: self.best_loss = self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annealer(f):\n",
    "    def _inner(start, end): return partial(f, start, end)\n",
    "    return _inner\n",
    "@annealer\n",
    "def sched_lin(start, end, pos): return start + pos*(end-start)\n",
    "@annealer\n",
    "def sched_cos(start, end, pos): return start + (1+math.cos(math.pi*(1-pos)))*(end-start)/2\n",
    "\n",
    "@annealer\n",
    "def sched_no(start, end, pos): return start\n",
    "\n",
    "@annealer\n",
    "def sched_exp(start, end, pos): return start*(end/start)**pos\n",
    "\n",
    "def combine_scheds(pcts, scheds):\n",
    "    assert sum(pcts)==1.\n",
    "    pcts = torch.tensor([0] + listify(pcts))\n",
    "    assert torch.all(pcts>=0)\n",
    "    pcts = torch.cumsum(pcts, 0)\n",
    "    def _inner(pos):\n",
    "        idx = (pos>=pcts).nonzero().max()\n",
    "        actual_pos = (pos-pcts[idx])/(pcts[idx+1]-pcts[idx])\n",
    "        return scheds[idx](actual_pos)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_model=lambda l: sum(map(flatten_model,l.children()),[]) if num_children(l) else [l]\n",
    "def children(m:nn.Module)->ModuleList:\n",
    "    \"Get children of module\"\n",
    "    return list(m.children())\n",
    "def num_children(m:nn.Module)->int:\n",
    "    \"Get number of child modules in module\"\n",
    "    return len(children(m))\n",
    "def ifnone(a:bool,b:Any):\n",
    "    \"`a` if its not None, otherwise `b`\"\n",
    "    return b if a is None else a\n",
    "def is_listy(x:Any)->bool: return isinstance(x, (tuple,list))\n",
    "def to_device(b:Tensors, device:torch.device):\n",
    "    \"Ensure `b` is on `device`\"\n",
    "    device = ifnone(device, default_device)\n",
    "    if is_listy(b): return [to_device(o, device) for o in b]\n",
    "    return b.to(device)\n",
    "\n",
    "\n",
    "AdamW = partial(optim.Adam, betas=(0.9,0.99)) \n",
    "default_lr = slice(3e-3)\n",
    "default_wd = 1e-2\n",
    "@dataclass\n",
    "class Learner():\n",
    "    \"Object that wraps together some data, a model, a loss function and an optimizer\"\n",
    "    data:DataBunch\n",
    "    model:nn.Module\n",
    "    opt_fn:Callable=AdamW\n",
    "    loss_fn:Callable=F.cross_entropy\n",
    "    metrics:Collection[Callable]=None\n",
    "    true_wd:bool=True\n",
    "    bn_wd:bool=True\n",
    "    wd:Floats=default_wd\n",
    "    train_bn:bool=True\n",
    "    path:str = None\n",
    "    model_dir:str = 'models'\n",
    "    callback_fns:Collection[Callable]=None\n",
    "    callbacks:Collection[Callback]=field(default_factory=list)\n",
    "    layer_groups:Collection[nn.Module]=None\n",
    "    def __post_init__(self)->None:\n",
    "        \"Setup path,metrics, callbacks and ensure model directory exists\"\n",
    "        self.path = Path(ifnone(self.path, self.data.path))\n",
    "        (self.path/self.model_dir).mkdir(parents=True, exist_ok=True)\n",
    "        self.model = self.model.to(self.data.device)\n",
    "        self.metrics=listify(self.metrics)\n",
    "        if not self.layer_groups: self.layer_groups = [nn.Sequential(*flatten_model(self.model))]\n",
    "        self.callbacks = listify(self.callbacks)\n",
    "        self.callback_fns = [Recorder] + listify(self.callback_fns)\n",
    "\n",
    "    def lr_range(self, lr:Union[float,slice])->np.ndarray:\n",
    "        \"Build learning rate schedule\"\n",
    "        if not isinstance(lr,slice): return lr\n",
    "        if lr.start: res = even_mults(lr.start, lr.stop, len(self.layer_groups))\n",
    "        else: res = [lr.stop/3]*(len(self.layer_groups)-1) + [lr.stop]\n",
    "        return np.array(res)\n",
    "\n",
    "    def fit(self, epochs:int, lr:Union[Floats,slice]=default_lr,\n",
    "            wd:Floats=None, callbacks:Collection[Callback]=None)->None:\n",
    "        \"fit the model on this learner with `lr` learning rate, `wd` weight decay for `epochs` with `callbacks`\"\n",
    "        lr = self.lr_range(lr)\n",
    "        if wd is None: wd = self.wd\n",
    "        self.create_opt(lr, wd)\n",
    "        callbacks = [cb(self) for cb in self.callback_fns] + listify(callbacks)\n",
    "        fit(epochs, self.model, self.loss_fn, opt=self.opt, data=self.data, metrics=self.metrics,\n",
    "            callbacks=self.callbacks+callbacks)\n",
    "\n",
    "    def create_opt(self, lr:Floats, wd:Floats=0.)->None:\n",
    "        \"create optimizer with `lr` learning rate and `wd` weight decay\"\n",
    "        self.opt = OptimWrapper.create(self.opt_fn, lr, self.layer_groups, wd=wd, true_wd=self.true_wd, bn_wd=self.bn_wd)\n",
    "\n",
    "    def split(self, split_on:SplitFuncOrIdxList)->None:\n",
    "        \"split the model at `split_on`\"\n",
    "        if isinstance(split_on,Callable): self.layer_groups = split_on(self.model)\n",
    "        else: self.layer_groups = split_model(self.model, split_on)\n",
    "\n",
    "    def freeze_to(self, n:int)->None:\n",
    "        \"freeze layers up to layer `n`\"\n",
    "        for g in self.layer_groups[:n]:\n",
    "            for l in g:\n",
    "                if not self.train_bn or not isinstance(l, bn_types): requires_grad(l, False)\n",
    "        for g in self.layer_groups[n:]: requires_grad(g, True)\n",
    "\n",
    "    def freeze(self)->None:\n",
    "        \"freeze up to last layer\"\n",
    "        assert(len(self.layer_groups)>1)\n",
    "        self.freeze_to(-1)\n",
    "\n",
    "    def unfreeze(self):\n",
    "        \"unfreeze entire model\"\n",
    "        self.freeze_to(0)\n",
    "    def __del__(self): del(self.model, self.data)\n",
    "    def save(self, name:PathOrStr):\n",
    "        \"save model with `name` to `self.model_dir`\"\n",
    "        torch.save(self.model.state_dict(), self.path/self.model_dir/f'{name}.pth')\n",
    "    def load(self, name:PathOrStr):\n",
    "        \"load model `name` from `self.model_dir\"\n",
    "        self.model.load_state_dict(torch.load(self.path/self.model_dir/f'{name}.pth'))\n",
    "\n",
    "def fit_one_cycle(learn:Learner, cyc_len:int,\n",
    "                  max_lr:Union[Floats,slice]=default_lr, moms:Tuple[float,float]=(0.95,0.85),\n",
    "                  div_factor:float=25., pct_start:float=0.3, wd:float=None, **kwargs)->None:\n",
    "    \"Fits a model following the 1cycle policy\"\n",
    "    max_lr = learn.lr_range(max_lr)\n",
    "    cbs = [OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor,\n",
    "                             pct_start=pct_start, **kwargs)]\n",
    "    learn.fit(cyc_len, max_lr, wd=wd, callbacks=cbs)\n",
    "\n",
    "\n",
    "def lr_find(learn:Learner, start_lr:float=1e-5, end_lr:float=10, num_it:int=100, **kwargs:Any):\n",
    "    \"Explore lr from `start_lr` to `end_lr` over `num_it` iterations of `learn`\"\n",
    "    cb = LRFinder(learn, start_lr, end_lr, num_it)\n",
    "    a = int(np.ceil(num_it/len(learn.data.train_dl)))\n",
    "    learn.fit(a, start_lr, callbacks=[cb], **kwargs)\n",
    "    \n",
    "Learner.fit_one_cycle = fit_one_cycle\n",
    "Learner.lr_find = lr_find\n",
    "        \n",
    "def dropout_mask(x:Tensor, sz:Collection[int], p:float):\n",
    "    \"Returns a dropout mask of the same type as x, size sz, with probability p to cancel an element.\"\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)\n",
    "\n",
    "class RNNDropout(nn.Module):\n",
    "    \"Dropout that is consistent on the seq_len dimension\"\n",
    "    \n",
    "    def __init__(self, p:float=0.5):\n",
    "        super().__init__()\n",
    "        self.p=p\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        if not self.training or self.p == 0.: return x\n",
    "        m = dropout_mask(x.data, (1, x.size(1), x.size(2)), self.p)\n",
    "        return x * m\n",
    "    \n",
    "class WeightDropout(nn.Module):\n",
    "    \"A module that warps another layer in which some weights will be replaced by 0 during training.\"\n",
    "    \n",
    "    def __init__(self, module:Model, weight_p:float, layer_names:Collection[str]=['weight_hh_l0']):\n",
    "        super().__init__()\n",
    "        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "    \n",
    "    def _setweights(self):\n",
    "        \"Applies dropout to the raw weights\"\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "            \n",
    "    def forward(self, *args:ArgStar):\n",
    "        self._setweights()\n",
    "        with warnings.catch_warnings():\n",
    "            #To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)\n",
    "    \n",
    "    def reset(self):\n",
    "        if hasattr(self.module, 'reset'): self.module.reset()\n",
    "            \n",
    "class EmbeddingDropout(nn.Module):\n",
    "    \"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n",
    "    \n",
    "    def __init__(self, emb:Model, embed_p:float):\n",
    "        super().__init__()\n",
    "        self.emb,self.embed_p = emb,embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "\n",
    "    def forward(self, words:LongTensor, scale:Optional[float]=None) -> Tensor:\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0),1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            masked_embed = self.emb.weight * mask\n",
    "        else: masked_embed = self.emb.weight\n",
    "        if scale: masked_embed.mul_(scale)\n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n",
    "    \n",
    "def repackage_var(h:Tensors) -> Tensors:\n",
    "    \"Detaches h from its history.\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(repackage_var(v) for v in h)\n",
    "\n",
    "class RNNCore(nn.Module):\n",
    "    \"AWD-LSTM/QRNN inspired by https://arxiv.org/abs/1708.02182\"\n",
    "\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz:int, emb_sz:int, n_hid:int, n_layers:int, pad_token:int, bidir:bool=False,\n",
    "                 hidden_p:float=0.2, input_p:float=0.6, embed_p:float=0.1, weight_p:float=0.5, qrnn:bool=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.bs,self.qrnn,self.ndir = 1, qrnn,(2 if bidir else 1)\n",
    "        self.emb_sz,self.n_hid,self.n_layers = emb_sz,n_hid,n_layers\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
    "        if self.qrnn:\n",
    "            #Using QRNN requires cupy: https://github.com/cupy/cupy\n",
    "            from qrnn import QRNNLayer\n",
    "            self.rnns = [QRNNLayer(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                                   save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True, \n",
    "                                   use_cuda=torch.cuda.is_available()) for l in range(n_layers)]\n",
    "            if weight_p != 0.:\n",
    "                for rnn in self.rnns:\n",
    "                    rnn.linear = WeightDropout(rnn.linear, weight_p, layer_names=['weight'])\n",
    "        else:\n",
    "            self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                1, bidirectional=bidir) for l in range(n_layers)]\n",
    "            if weight_p != 0.: self.rnns = [WeightDropout(rnn, weight_p) for rnn in self.rnns]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input:LongTensor) -> Tuple[Tensor,Tensor]:\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.encoder_dp(input))\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = repackage_var(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def one_hidden(self, l:int) -> Tensor:\n",
    "        \"Returns one hidden state\"\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz)//self.ndir\n",
    "        return self.weights.new(self.ndir, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Resets the hidden states\"\n",
    "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self.one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self.one_hidden(l), self.one_hidden(l)) for l in range(self.n_layers)]\n",
    "            \n",
    "class LinearDecoder(nn.Module):\n",
    "    \"To go on top of a RNN_Core module\"\n",
    "    \n",
    "    initrange=0.1\n",
    "    \n",
    "    def __init__(self, n_out:int, n_hid:int, output_p:float, tie_encoder:Model=None, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor]) -> Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1])\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialRNN(nn.Sequential):\n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_model(vocab_sz:int, emb_sz:int, n_hid:int, n_layers:int, pad_token:int, tie_weights:bool=True, \n",
    "                       qrnn:bool=False, bias:bool=True, output_p:float=0.4, hidden_p:float=0.2, input_p:float=0.6, \n",
    "                       embed_p:float=0.1, weight_p:float=0.5) -> Model:\n",
    "    \"To create a full AWD-LSTM\"\n",
    "    rnn_enc = RNNCore(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token, qrnn=qrnn,\n",
    "                 hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    enc = rnn_enc.encoder if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GradientClipping(Callback):\n",
    "    \"To do gradient clipping during training.\"\n",
    "    learn:Learner\n",
    "    clip:float\n",
    "\n",
    "    def on_backward_end(self, **kwargs):\n",
    "        if self.clip:  nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)\n",
    "            \n",
    "@dataclass\n",
    "class RNNTrainer(Callback):\n",
    "    \"`Callback` that regroups lr adjustment to seq_len, AR and TAR\"\n",
    "    learn:Learner\n",
    "    bptt:int\n",
    "    alpha:float=0.\n",
    "    beta:float=0.\n",
    "    adjust:bool=True\n",
    "    \n",
    "    def on_loss_begin(self, last_output:Tuple[Tensor,Tensor,Tensor], **kwargs):\n",
    "        #Save the extra outputs for later and only returns the true output.\n",
    "        self.raw_out,self.out = last_output[1],last_output[2]\n",
    "        return last_output[0]\n",
    "    \n",
    "    def on_backward_begin(self, last_loss:Rank0Tensor, last_input:Tensor, last_output:Tensor, **kwargs):\n",
    "        #Adjusts the lr to the bptt selected\n",
    "        if self.adjust: self.learn.opt.lr *= last_input.size(0) / self.bptt\n",
    "        #AR and TAR\n",
    "        if self.alpha != 0.:  last_loss += (self.alpha * self.out[-1].pow(2).mean()).sum()\n",
    "        if self.beta != 0.:\n",
    "            h = self.raw_out[-1]\n",
    "            if len(h)>1: last_loss += (self.beta * (h[1:] - h[:-1]).pow(2).mean()).sum()\n",
    "        return last_loss\n",
    "    \n",
    "@dataclass\n",
    "class OneCycleScheduler(Callback):\n",
    "    \"Manages 1-Cycle style traing as outlined in Leslie Smith's [paper](https://arxiv.org/pdf/1803.09820.pdf)\"\n",
    "    learn:Learner\n",
    "    lr_max:float\n",
    "    moms:Floats=(0.95,0.85)\n",
    "    div_factor:float=25.\n",
    "    pct_start:float=0.5\n",
    "\n",
    "    def __post_init__(self): self.moms=tuple(listify(self.moms,2))\n",
    "\n",
    "    def steps(self, *steps_cfg:StartOptEnd):\n",
    "        \"Build anneal schedule for all of the parameters\"\n",
    "        return [Stepper(step, n_iter, func=func)\n",
    "                for (step,(n_iter,func)) in zip(steps_cfg, self.phases)]\n",
    "\n",
    "    def on_train_begin(self, n_epochs:int, **kwargs:Any)->None:\n",
    "        \"Initialize our optimization params based on our annealing schedule\"\n",
    "        n = len(self.learn.data.train_dl) * n_epochs\n",
    "        a1 = int(n * self.pct_start)\n",
    "        a2 = n-a1\n",
    "        self.phases = ((a1, annealing_linear), (a2, annealing_cos))\n",
    "        low_lr = self.lr_max/self.div_factor\n",
    "        self.lr_scheds = self.steps((low_lr, self.lr_max), (self.lr_max, low_lr/1e4))\n",
    "        self.mom_scheds = self.steps(self.moms, (self.moms[1], self.moms[0]))\n",
    "        self.opt = self.learn.opt\n",
    "        self.opt.lr,self.opt.mom = self.lr_scheds[0].start,self.mom_scheds[0].start\n",
    "        self.idx_s = 0\n",
    "\n",
    "    def on_batch_end(self, **kwargs:Any)->None:\n",
    "        \"Take one step forward on the annealing schedule for the optim params\"\n",
    "        if self.idx_s >= len(self.lr_scheds): return Trrue\n",
    "        self.opt.lr = self.lr_scheds[self.idx_s].step()\n",
    "        self.opt.mom = self.mom_scheds[self.idx_s].step()\n",
    "        # when the current schedule is complete we move onto the next\n",
    "        # schedule. (in 1-cycle there are two schedules)\n",
    "        if self.lr_scheds[self.idx_s].is_done:\n",
    "            self.idx_s += 1\n",
    "\n",
    "def split_bn_bias(layer_groups:ModuleList)->ModuleList:\n",
    "    \"Sort each layer in  `layer_groups` into batchnorm (`bn_types`) and non-batchnorm groups\"\n",
    "    split_groups = []\n",
    "    for l in layer_groups:\n",
    "        l1,l2 = [],[]\n",
    "        for c in l.children():\n",
    "            if isinstance(c, bn_types): l2.append(c)\n",
    "            else:                       l1.append(c)\n",
    "        split_groups += [nn.Sequential(*l1), nn.Sequential(*l2)]\n",
    "    return split_groups\n",
    "def trainable_params(m:nn.Module)->ParamList:\n",
    "    \"Return list of trainable params in `m`\"\n",
    "    res = filter(lambda p: p.requires_grad, m.parameters())\n",
    "    return res\n",
    "class OptimWrapper():\n",
    "    \"Basic wrapper around an optimizer to simplify HP changes\"\n",
    "    def __init__(self, opt:optim.Optimizer, wd:Floats=0., true_wd:bool=False, bn_wd:bool=True)->None:\n",
    "        self.opt,self.true_wd,self.bn_wd = opt,true_wd,bn_wd\n",
    "        self.opt_keys = list(self.opt.param_groups[0].keys())\n",
    "        self.opt_keys.remove('params')\n",
    "        self.read_defaults()\n",
    "        self.wd = wd\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, opt_fn:Union[type,Callable], lr:Union[float,Tuple,List],\n",
    "               layer_groups:ModuleList, **kwargs:Any)->optim.Optimizer:\n",
    "        \"Create an optim.Optimizer from `opt_fn` with `lr`. Set lr on `layer_groups``\"\n",
    "        split_groups = split_bn_bias(layer_groups)\n",
    "        opt = opt_fn([{'params': trainable_params(l), 'lr':0} for l in split_groups])\n",
    "        opt = cls(opt, **kwargs)\n",
    "        opt.lr = listify(lr, layer_groups)\n",
    "        return opt\n",
    "\n",
    "    def __repr__(self)->str:\n",
    "        return f'OptimWrapper over {repr(self.opt)}.\\nTrue weight decay: {self.true_wd}'\n",
    "\n",
    "    #Pytorch optimizer methods\n",
    "    def step(self)->None:\n",
    "        \"Set weight decay and step optimizer\"\n",
    "        # weight decay outside of optimizer step (AdamW)\n",
    "        if self.true_wd:\n",
    "            for lr,wd,pg1,pg2 in zip(self._lr,self._wd,self.opt.param_groups[::2],self.opt.param_groups[1::2]):\n",
    "                for p in pg1['params']: p.data.mul_(1 - wd*lr)\n",
    "                if self.bn_wd:\n",
    "                    for p in pg2['params']: p.data.mul_(1 - wd*lr)\n",
    "            self.set_val('weight_decay', listify(0, self._wd))\n",
    "        self.opt.step()\n",
    "\n",
    "    def zero_grad(self)->None:\n",
    "        \"Clear optimizer gradients\"\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "    #Hyperparameters as properties\n",
    "    @property\n",
    "    def lr(self)->float:\n",
    "        \"Get learning rate\"\n",
    "        return self._lr[-1]\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, val:float)->None:\n",
    "        \"Set learning rate\"\n",
    "        self._lr = self.set_val('lr', listify(val, self._lr))\n",
    "\n",
    "    @property\n",
    "    def mom(self)->float:\n",
    "        \"Get momentum\"\n",
    "        return self._mom[-1]\n",
    "\n",
    "    @mom.setter\n",
    "    def mom(self, val:float)->None:\n",
    "        \"Set momentum\"\n",
    "        if 'momentum' in self.opt_keys: self.set_val('momentum', listify(val, self._mom))\n",
    "        elif 'betas' in self.opt_keys:  self.set_val('betas', (listify(val, self._mom), self._beta))\n",
    "        self._mom = listify(val, self._mom)\n",
    "\n",
    "    @property\n",
    "    def beta(self)->float:\n",
    "        \"get beta\"\n",
    "        return None if self._beta is None else self._beta[-1]\n",
    "\n",
    "    @beta.setter\n",
    "    def beta(self, val:float)->None:\n",
    "        \"Set beta (or alpha as makes sense for give optimizer)\"\n",
    "        if val is None: return\n",
    "        if 'betas' in self.opt_keys:    self.set_val('betas', (self._mom, listify(val, self._beta)))\n",
    "        elif 'alpha' in self.opt_keys:  self.set_val('alpha', listify(val, self._beta))\n",
    "        self._beta = listify(val, self._beta)\n",
    "\n",
    "    @property\n",
    "    def wd(self)->float:\n",
    "        \"Get weight decay\"\n",
    "        return self._wd[-1]\n",
    "\n",
    "    @wd.setter\n",
    "    def wd(self, val:float)->None:\n",
    "        \"Set weight decay\"\n",
    "        if not self.true_wd: self.set_val('weight_decay', listify(val, self._wd), bn_groups=self.bn_wd)\n",
    "        self._wd = listify(val, self._wd)\n",
    "\n",
    "    #Helper functions\n",
    "    def read_defaults(self)->None:\n",
    "        \"Read the values inside the optimizer for the hyper-parameters\"\n",
    "        self._beta = None\n",
    "        if 'lr' in self.opt_keys: self._lr = self.read_val('lr')\n",
    "        if 'momentum' in self.opt_keys: self._mom = self.read_val('momentum')\n",
    "        if 'alpha' in self.opt_keys: self._beta = self.read_val('alpha')\n",
    "        if 'betas' in self.opt_keys: self._mom,self._beta = self.read_val('betas')\n",
    "        if 'weight_decay' in self.opt_keys: self._wd = self.read_val('weight_decay')\n",
    "\n",
    "    def set_val(self, key:str, val:Any, bn_groups:bool=True)->Any:\n",
    "        \"Set the values inside the optimizer dictionary at the key\"\n",
    "        if is_tuple(val): val = [(v1,v2) for v1,v2 in zip(*val)]\n",
    "        for v,pg1,pg2 in zip(val,self.opt.param_groups[::2],self.opt.param_groups[1::2]):\n",
    "            pg1[key] = v\n",
    "            if bn_groups: pg2[key] = v\n",
    "        return val\n",
    "\n",
    "    def read_val(self, key:str) -> Union[List[float],Tuple[List[float],List[float]]]:\n",
    "        \"Read a hyper-parameter key in the optimizer dictionary.\"\n",
    "        val = [pg[key] for pg in self.opt.param_groups[::2]]\n",
    "        if is_tuple(val[0]): val = [o[0] for o in val], [o[1] for o in val]\n",
    "        return val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342834,)\n",
      "(2678, 128)\n",
      "(117308,)\n",
      "(916, 128)\n"
     ]
    }
   ],
   "source": [
    "trn_dl = LanguageModelLoader(data_trn_,bs=128,bptt=80)\n",
    "tst_dl = LanguageModelLoader(data_tst_,bs=128,bptt=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(trn_dl,tst_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2000,\n",
       " 1: 2010,\n",
       " 2: 2020,\n",
       " 3: 2030,\n",
       " 4: 3010,\n",
       " 5: 3020,\n",
       " 6: 3021,\n",
       " 7: 3110,\n",
       " 8: 3120,\n",
       " 9: 3121,\n",
       " 10: 4010,\n",
       " 11: 4020,\n",
       " 12: 4050,\n",
       " 13: 4070,\n",
       " 14: 4090,\n",
       " 15: 9998,\n",
       " 16: 9999}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz, nh, nl = 50, 120, 2\n",
    "vocab_size = len(index2vocab)\n",
    "model = get_language_model(vocab_size, emb_sz, nh, nl, 0, input_p=0.6, output_p=0.4, weight_p=0.5, \n",
    "                           embed_p=0.1, hidden_p=0.2)\n",
    "learn = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 20,10\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.8,0.99))\n",
    "learn.callbacks.append(RNNTrainer(learn, bptt, alpha=2, beta=1))\n",
    "learn.callback_fns = [partial(GradientClipping, clip=0.12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-c3f34e23d4a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.2e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-9e26721f0c40>\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, wd, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     cbs = [OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor,\n\u001b[1;32m    104\u001b[0m                              pct_start=pct_start, **kwargs)]\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-9e26721f0c40>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         fit(epochs, self.model, self.loss_fn, opt=self.opt, data=self.data, metrics=self.metrics,\n\u001b[0m\u001b[1;32m     64\u001b[0m             callbacks=self.callbacks+callbacks)\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fit' is not defined"
     ]
    }
   ],
   "source": [
    "fit_one_cycle(learn, 1, 5e-3, (0.8,0.7), wd=1.2e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([70, 128]), torch.Size([8960]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0].shape,d[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([70, 128, 17]), torch.Size([8960]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape,d[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = d[1].view(70,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7939, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(o.view(-1,17),d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval():\n",
    "    #test_preds = np.zeros(len(test_x))\n",
    "    #for fold, (train_idx, valid_idx) in enumerate(splits):\n",
    "        #print('Fold:', fold)\n",
    "    torch.cuda.empty_cache()\n",
    "    learn = get_learner(train_idx, valid_idx)\n",
    "    gc.collect()\n",
    "    run = Runner(cb_funcs=cbfs)\n",
    "    learn.model.train()\n",
    "    run.fit(4, learn)\n",
    "    learn.model.eval()\n",
    "    test_preds_fold = np.zeros(len(test_dl.dataset))\n",
    "    for i, (x_batch,) in enumerate(test_dl):\n",
    "        with torch.no_grad():\n",
    "            y_pred = learn.model(x_batch).detach()\n",
    "        test_preds_fold[i*batch_size:(i+1)*batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "    test_preds+=test_preds_fold/len(splits)\n",
    "    del(learn)\n",
    "    gc.collect()\n",
    "    print(f'Test {fold} added')\n",
    "    print('Training Completed')\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_learner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e06a98e4c5fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-c11f7c779e82>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#print('Fold:', fold)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb_funcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_learner' is not defined"
     ]
    }
   ],
   "source": [
    "train_and_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval():\n",
    "    test_preds = np.zeros(len(test_x))\n",
    "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print('Fold:', fold)\n",
    "        torch.cuda.empty_cache()\n",
    "        learn = get_learner(train_idx, valid_idx)\n",
    "        gc.collect()\n",
    "        run = Runner(cb_funcs=cbfs)\n",
    "        learn.model.train()\n",
    "        run.fit(4, learn)\n",
    "        learn.model.eval()\n",
    "        test_preds_fold = np.zeros(len(test_dl.dataset))\n",
    "        for i, (x_batch,) in enumerate(test_dl):\n",
    "            with torch.no_grad():\n",
    "                y_pred = learn.model(x_batch).detach()\n",
    "            test_preds_fold[i*batch_size:(i+1)*batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        test_preds+=test_preds_fold/len(splits)\n",
    "        del(learn)\n",
    "        gc.collect()\n",
    "        print(f'Test {fold} added')\n",
    "    print('Training Completed')\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
